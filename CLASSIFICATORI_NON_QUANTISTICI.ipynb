{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Questo script ha lo scopo di addestrare sullo stesso dataset alcuni classificatori classici e stamparne le stesse metriche di valutazione usate per il caso quantistico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wWHS36J5aqME",
        "outputId": "6fedbd47-edd1-4579-b48a-d81d86527842"
      },
      "outputs": [],
      "source": [
        "# Installazione delle librerie necessarie\n",
        "!pip install sklearn\n",
        "!pip install pandas\n",
        "!pip install seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "import time\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                           f1_score, confusion_matrix, classification_report)\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# _Stessa logica di gestione del caso precedente, main centralizzato che gestisce il tutto_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Impostazioni per i plot\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Funzione main che esegue tutto il pipeline di classificazione classica\n",
        "    \"\"\"\n",
        "\n",
        "    # =============================================================================\n",
        "    # 1. CARICAMENTO E PREPROCESSING DEL DATASET WINE CON FEATURE SELECTION\n",
        "    # =============================================================================\n",
        "\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CARICAMENTO DATASET WINE E PREPROCESSING CON PCA\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Caricamento dataset Wine\n",
        "    wine_data = load_wine()\n",
        "\n",
        "    features = wine_data.data\n",
        "    labels = wine_data.target\n",
        "    feature_names = wine_data.feature_names\n",
        "    target_names = wine_data.target_names\n",
        "\n",
        "    # Standardizzazione per PCA \n",
        "    print(\"\\n\" + \"-\" * 50)\n",
        "    print(\"STANDARDIZZAZIONE E PCA\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "\n",
        "    pca = PCA(n_components=5)\n",
        "    features_pca = pca.fit_transform(features_scaled)\n",
        "\n",
        "\n",
        "    # Normalizzazione delle componenti PCA per i circuiti quantistici\n",
        "    normalizer = MinMaxScaler()\n",
        "    features_pca5_normalized = normalizer.fit_transform(features_pca)\n",
        "\n",
        "\n",
        "    # Split train/test\n",
        "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
        "        features_pca5_normalized, labels, train_size=0.8, random_state=123, stratify=labels\n",
        "    )\n",
        "\n",
        "    # =============================================================================\n",
        "    # 2. ESECUZIONE ESPERIMENTI CLASSICI\n",
        "    # =============================================================================\n",
        "\n",
        "    print(f\"\\n\" + \"=\" * 80)\n",
        "    print(\"AVVIO ESPERIMENTI CLASSIFICATORI CLASSICI\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Iniziando esperimenti con {features_pca5_normalized.shape[1]} features\")\n",
        "    print(f\"Dataset: {len(train_features)} train samples, {len(test_features)} test samples\")\n",
        "    print(f\"Classi da predire: {len(target_names)} ({', '.join(target_names)})\")\n",
        "\n",
        "    classical_results = run_classical_experiments(\n",
        "        train_features, train_labels, test_features, test_labels, target_names\n",
        "    )\n",
        "\n",
        "    # =============================================================================\n",
        "    # 3. ANALISI COMPLETA DEI RISULTATI\n",
        "    # =============================================================================\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ANALISI FINALE CLASSIFICATORI CLASSICI\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Analisi dei risultati classici\n",
        "    analyze_classical_results(classical_results)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ESPERIMENTI CLASSICI COMPLETATI CON SUCCESSO!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return classical_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## _Funzione che avvia gli esperimenti classici sfruttando 6 calssificatori_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def run_classical_experiments(train_features, train_labels, test_features, test_labels, target_names):\n",
        "    \"\"\"Esegue tutti gli esperimenti comparativi con classificatori classici\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ESPERIMENTI COMPARATIVI CLASSIFICATORI CLASSICI CON METRICHE DETTAGLIATE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Definiamo i classificatori classici\n",
        "    classifiers = {\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=123),\n",
        "        'Support Vector Machine': SVC(kernel='rbf', random_state=123, probability=True),\n",
        "        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(random_state=123),\n",
        "        'Logistic Regression': LogisticRegression(random_state=123, max_iter=1000),\n",
        "        'Naive Bayes': GaussianNB()\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "    experiment_count = 0\n",
        "    total_experiments = len(classifiers)\n",
        "\n",
        "    print(f\"Totale esperimenti da eseguire: {total_experiments}\")\n",
        "    print(f\"Numero di features per esperimento: {train_features.shape[1]}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for clf_name, classifier in classifiers.items():\n",
        "        experiment_count += 1\n",
        "        print(f\"\\n{'='*20} ESPERIMENTO {experiment_count}/{total_experiments} {'='*20}\")\n",
        "        print(f\"Classificatore: {clf_name}\")\n",
        "\n",
        "        # Stampa dei parametri del modello se disponibili\n",
        "        if hasattr(classifier, 'get_params'):\n",
        "            params = classifier.get_params()\n",
        "            key_params = []\n",
        "            if 'n_estimators' in params:\n",
        "                key_params.append(f\"n_estimators={params['n_estimators']}\")\n",
        "            if 'kernel' in params:\n",
        "                key_params.append(f\"kernel={params['kernel']}\")\n",
        "            if 'n_neighbors' in params:\n",
        "                key_params.append(f\"n_neighbors={params['n_neighbors']}\")\n",
        "            if key_params:\n",
        "                print(f\"Parametri principali: {', '.join(key_params)}\")\n",
        "\n",
        "        # Addestramento\n",
        "        print(\"Addestramento in corso...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        classifier.fit(train_features, train_labels)\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Valutazione\n",
        "        train_pred = classifier.predict(train_features)\n",
        "        test_pred = classifier.predict(test_features)\n",
        "\n",
        "        train_acc = accuracy_score(train_labels, train_pred)\n",
        "        test_acc = accuracy_score(test_labels, test_pred)\n",
        "\n",
        "        # Calcolo metriche dettagliate\n",
        "        test_precision = precision_score(test_labels, test_pred, average='weighted')\n",
        "        test_recall = recall_score(test_labels, test_pred, average='weighted')\n",
        "        test_f1 = f1_score(test_labels, test_pred, average='weighted')\n",
        "\n",
        "        print(f\"RISULTATI:\")\n",
        "        print(f\"Train Accuracy: {train_acc:.4f}\")\n",
        "        print(f\"Test Accuracy:  {test_acc:.4f}\")\n",
        "        print(f\"Test Precision: {test_precision:.4f}\")\n",
        "        print(f\"Test Recall:    {test_recall:.4f}\")\n",
        "        print(f\"Test F1-Score:  {test_f1:.4f}\")\n",
        "        print(f\"Training Time:  {training_time:.4f}s\")\n",
        "\n",
        "        # Confusion Matrix per questo esperimento\n",
        "        experiment_title = f\"{clf_name}\"\n",
        "        print(f\"\\n CONFUSION MATRIX - {experiment_title}\")\n",
        "\n",
        "        cm = confusion_matrix(test_labels, test_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=target_names, yticklabels=target_names)\n",
        "        plt.title(f'Confusion Matrix\\n{experiment_title}')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.show()\n",
        "\n",
        "        # Classification report dettagliato\n",
        "        print(f\"\\n CLASSIFICATION REPORT - {experiment_title}\")\n",
        "        print(classification_report(test_labels, test_pred, target_names=target_names))\n",
        "\n",
        "        # Salvataggio risultati\n",
        "        result = {\n",
        "            'classifier': clf_name,\n",
        "            'train_accuracy': train_acc,\n",
        "            'test_accuracy': test_acc,\n",
        "            'test_precision': test_precision,\n",
        "            'test_recall': test_recall,\n",
        "            'test_f1': test_f1,\n",
        "            'training_time': training_time,\n",
        "            'success': True\n",
        "        }\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# _Funzione che plotta e analizza i risultati ottenuti e ricevuti dalla funzione precedente_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def analyze_classical_results(classical_results):\n",
        "    \"\"\"Analizza e visualizza i risultati degli esperimenti classici\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ANALISI AVANZATA DEI RISULTATI CLASSICI\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Conversione in DataFrame\n",
        "    df_results = pd.DataFrame(classical_results)\n",
        "\n",
        "    # Statistiche generali\n",
        "    print(\"\\n1. STATISTICHE GENERALI\")\n",
        "    print(\"-\" * 40)\n",
        "    successful_experiments = df_results[df_results['success'] == True]\n",
        "    print(f\"Esperimenti riusciti: {len(successful_experiments)}/{len(df_results)}\")\n",
        "    print(f\"Accuracy media (train): {successful_experiments['train_accuracy'].mean():.4f} ± {successful_experiments['train_accuracy'].std():.4f}\")\n",
        "    print(f\"Accuracy media (test): {successful_experiments['test_accuracy'].mean():.4f} ± {successful_experiments['test_accuracy'].std():.4f}\")\n",
        "    print(f\"Precision media (test): {successful_experiments['test_precision'].mean():.4f} ± {successful_experiments['test_precision'].std():.4f}\")\n",
        "    print(f\"Recall media (test): {successful_experiments['test_recall'].mean():.4f} ± {successful_experiments['test_recall'].std():.4f}\")\n",
        "    print(f\"F1-Score media (test): {successful_experiments['test_f1'].mean():.4f} ± {successful_experiments['test_f1'].std():.4f}\")\n",
        "    print(f\"Tempo medio di training: {successful_experiments['training_time'].mean():.4f}s ± {successful_experiments['training_time'].std():.4f}s\")\n",
        "\n",
        "    # Top configurazioni per test accuracy\n",
        "    print(\"\\n2. RANKING CLASSIFICATORI (Test Accuracy)\")\n",
        "    print(\"-\" * 80)\n",
        "    top_configs = successful_experiments.sort_values('test_accuracy', ascending=False)\n",
        "    print(f\"{'Rank':<4} {'Classifier':<25} {'Test Acc':<8} {'Precision':<9} {'Recall':<7} {'F1':<7} {'Time(s)':<8}\")\n",
        "    print(\"-\" * 80)\n",
        "    for rank, (idx, row) in enumerate(top_configs.iterrows(), 1):\n",
        "        print(f\"{rank:<4} {row['classifier']:<25} {row['test_accuracy']:<8.4f} {row['test_precision']:<9.4f} \"\n",
        "              f\"{row['test_recall']:<7.4f} {row['test_f1']:<7.4f} {row['training_time']:<8.4f}\")\n",
        "\n",
        "    # Grafico comparativo delle metriche\n",
        "    if len(successful_experiments) > 0:\n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "        # Subplot per accuracy\n",
        "        plt.subplot(2, 3, 1)\n",
        "        plt.bar(successful_experiments['classifier'], successful_experiments['test_accuracy'], color='skyblue')\n",
        "        plt.title('Test Accuracy per Classificatore')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Subplot per precision\n",
        "        plt.subplot(2, 3, 2)\n",
        "        plt.bar(successful_experiments['classifier'], successful_experiments['test_precision'], color='lightgreen')\n",
        "        plt.title('Test Precision per Classificatore')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Subplot per recall\n",
        "        plt.subplot(2, 3, 3)\n",
        "        plt.bar(successful_experiments['classifier'], successful_experiments['test_recall'], color='salmon')\n",
        "        plt.title('Test Recall per Classificatore')\n",
        "        plt.ylabel('Recall')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Subplot per F1-score\n",
        "        plt.subplot(2, 3, 4)\n",
        "        plt.bar(successful_experiments['classifier'], successful_experiments['test_f1'], color='gold')\n",
        "        plt.title('Test F1-Score per Classificatore')\n",
        "        plt.ylabel('F1-Score')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Subplot per training time\n",
        "        plt.subplot(2, 3, 5)\n",
        "        plt.bar(successful_experiments['classifier'], successful_experiments['training_time'], color='plum')\n",
        "        plt.title('Training Time per Classificatore')\n",
        "        plt.ylabel('Tempo (secondi)')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Subplot combinato per tutte le metriche\n",
        "        plt.subplot(2, 3, 6)\n",
        "        x = np.arange(len(successful_experiments))\n",
        "        width = 0.15\n",
        "\n",
        "        plt.bar(x - 2*width, successful_experiments['test_accuracy'], width, label='Accuracy', alpha=0.8)\n",
        "        plt.bar(x - width, successful_experiments['test_precision'], width, label='Precision', alpha=0.8)\n",
        "        plt.bar(x, successful_experiments['test_recall'], width, label='Recall', alpha=0.8)\n",
        "        plt.bar(x + width, successful_experiments['test_f1'], width, label='F1-Score', alpha=0.8)\n",
        "\n",
        "        plt.title('Confronto Tutte le Metriche')\n",
        "        plt.ylabel('Score')\n",
        "        plt.xticks(x, successful_experiments['classifier'], rotation=45, ha='right')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Miglior classificatore\n",
        "    if len(successful_experiments) > 0:\n",
        "        best_classifier = successful_experiments.loc[successful_experiments['test_accuracy'].idxmax()]\n",
        "\n",
        "        print(f\"\\n MIGLIOR CLASSIFICATORE:\")\n",
        "        print(f\"   Nome: {best_classifier['classifier']}\")\n",
        "        print(f\"   Test Accuracy: {best_classifier['test_accuracy']:.4f}\")\n",
        "        print(f\"   Precision: {best_classifier['test_precision']:.4f}\")\n",
        "        print(f\"   Recall: {best_classifier['test_recall']:.4f}\")\n",
        "        print(f\"   F1-Score: {best_classifier['test_f1']:.4f}\")\n",
        "        print(f\"   Training Time: {best_classifier['training_time']:.4f}s\")\n",
        "\n",
        "    return successful_experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Esecuzione del main\n",
        "if __name__ == \"__main__\":\n",
        "    classical_results = main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "quantum_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
